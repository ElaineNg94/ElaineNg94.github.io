<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f4feff">
  
  <title>Welcome to Elaine Ng's Portfolio!</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="https://andrewhnberry.github.io/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://andrewhnberry.github.io/css/main.css">



  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/79742633?s=400&u=f4fa51f4753abe7e803976555112f0fe67b6cd95&v=4/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/79742633?s=400&u=f4fa51f4753abe7e803976555112f0fe67b6cd95&v=4/favicon.png">
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  
    <img src="https://avatars.githubusercontent.com/u/79742633?v=4.jpg" alt="" class="avatar">
  </a>
  
  <h1 title="author_name">Elaine Ng</h1>
  <a href="https://andrewhnberry.github.io/" class="author_name">Elaine Ng</a>
  <span class="author_job">Seeking Data Analyst, Data Engineer, & Data Scientist positions in healthcare and tech industries</span>
  <span class="author_bio mbm">A little extension of my data science life!</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="https://andrewhnberry.github.io/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="https://andrewhnberry.github.io/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="https://andrewhnberry.github.io/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="https://andrewhnberry.github.io/resume/">Resume</a>
      </li>
        
      <li class="nav-item">
        <a href="https://andrewhnberry.github.io/tags/">Tags</a>
      </li>

    </ul>
  </nav>
  <script type="text/javascript">
  // based on http://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
    
    <li><a href="https://www.linkedin.com/in/elaineng94/" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    
    
    <li><a href="https://github.com/ElaineNg94" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">


<div id="post">
  <header class="post-header">
    <h1 title="About Me">About Me</h1>
    <p> Welcome to <strong>my</strong> portfolio! A little bit about me is I love exploring the world with an open mind, gaining new experiences, creating visualizations where I can analyze interesting data, and am constantly finding new topics to learn or practice with!</p>
    <p>Currently, seeking <strong>Data Analyst, Data Engineer, and Data Scientist</strong> positions with a Data Analytics certificate from <strong>UC Berkeley Extension</strong>. Experienced in Advanced Excel and VBA, Python, SQL, R, APIs, HTML, Tableau, Java, AWS RDS, Machine Learning, and more. Formerly an Administrative Assistant in the healthcare industry at a top hospital in the nation. Excellent skills in communication and collaboration, as well as a focus on meeting multiple deadlines. Eager to combine my background as a former Administrative Assistant with my technical skills to assist in data analysis, visualizations, and discover future data trends.</p>
 
      <img class="feature-image" src="https://user-images.githubusercontent.com/79742633/137049777-cd4c8015-1f45-4704-88c0-585bb8f8164d.png" alt="01-The Easy Way to Web Scrape Articles Online feature image">

      <div class='tableauPlaceholder' id='viz1636447668644' style='position: relative'><noscript><a href='#'><img alt='Tinder Success Rate Among Millennials Analysis ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;TinderSuccessRateAmongMillennialsStory&#47;Story1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='TinderSuccessRateAmongMillennialsStory&#47;Story1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;TinderSuccessRateAmongMillennialsStory&#47;Story1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1636447668644');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1016px';vizElement.style.height='991px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>

  </header>

  <article class="post-content">
    <h1 id="the-simplest-way-to-web-scrape-news-articles-online">The simplest way to web scrape news articles online!</h1>

<p>About a year ago, my final capstone project required me to <strong>web scrape a bunch of news articles online</strong>. Knowing my lazy self, I wanted to figure out the easiest way to achieve this task. There are tonnes of python web scraping plugins out there that could have helped me do the job, a popular one being <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a>. It’s a great plugin, however I did not want to get into the nitty gritty of understanding the unique html structures of each online news sites.</p>

<p><img src="/img/code/01_code.png" alt="Web Scraping" /></p>

<p>Through much googling (which is an important skill to know if you’re a developer), I did find a <strong>simple solution to my problems</strong>.  I found <a href="https://newspaper.readthedocs.io/en/latest/">Newspaper3k</a>!</p>

<p>Oh, did it save me so much time when parsing through an online news site to scrape that article. I’ll show you!</p>
<h2 id="how-to-use-newspaper3k-to-scrape-online-articles">How to use Newspaper3k to Scrape Online Articles</h2>

<p>First, we need to install the python plugin on your terminal. <em>Disclaimer: I’m using OSX</em></p>

<p><em>Pro tip: Do create another environment, it’s considered best practice.</em></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ pip install newspaper3k
</code></pre></div></div>

<h3 id="the-basics">The Basics</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="kn">import</span> <span class="nn">newspaper</span>
    <span class="kn">from</span> <span class="nn">newspaper</span> <span class="kn">import</span> <span class="n">Article</span>

    <span class="c1">#The Basics of downloading the article to memory
</span>    <span class="n">article</span> <span class="o">=</span> <span class="n">Article</span><span class="p">(</span><span class="s">"link to your article"</span><span class="p">)</span>
    <span class="n">article</span><span class="p">.</span><span class="n">download</span><span class="p">()</span>
    <span class="n">article</span><span class="p">.</span><span class="n">parse</span><span class="p">()</span>
    <span class="n">article</span><span class="p">.</span><span class="n">nlp</span><span class="p">()</span>

    <span class="c1">#To print out the full text
</span>    <span class="k">print</span><span class="p">(</span><span class="n">article</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="c1">#the text of the article should be printed out
</span>
    <span class="c1">#To print out the list of authors
</span>    <span class="n">article</span><span class="p">.</span><span class="n">authors</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="s">'author_1'</span><span class="p">,</span><span class="s">'author_2'</span><span class="p">]</span>

    <span class="c1">#To print out the list of keywords (thanks to the built in NLP functionalities)
</span>    <span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="s">'keywords_1'</span><span class="p">,</span><span class="s">'keywords_2'</span><span class="p">,.....</span><span class="n">etc</span>

    <span class="c1">#To print out a summary of the text
</span>    <span class="k">print</span><span class="p">(</span><span class="n">article</span><span class="p">.</span><span class="n">summary</span><span class="p">)</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="c1">#Summary of the text
</span>
    <span class="c1">#Other functions to gather the other bits of useful data in an article
</span>    <span class="n">article</span><span class="p">.</span><span class="n">title</span> <span class="c1">#Gives the title
</span>    <span class="n">article</span><span class="p">.</span><span class="n">top_image</span> <span class="c1">#Gives the link to the banner image, main image with the article
</span>    <span class="n">article</span><span class="p">.</span><span class="n">images</span> <span class="c1">#Provides a set of image links that could be saved and downloaded</span></code></pre></figure>

<h3 id="advanced-downloading-multiple-articles-from-one-news-site">Advanced: Downloading multiple articles from one news site.</h3>

<p>When I was scraping these articles, I wanted to scrape a bunch of articles from one news site and put everything in a pandas DataFrame so that I could export data out to a csv file. Sounds like a simple task right? You betcha!</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="kn">import</span> <span class="nn">newspaper</span>
    <span class="kn">from</span> <span class="nn">newspaper</span> <span class="kn">import</span> <span class="n">Article</span>
    <span class="kn">from</span> <span class="nn">newspaper</span> <span class="kn">import</span> <span class="n">Source</span>
    <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

    <span class="c1">#Lets say we wanted to download articles from GameSpot
</span>    <span class="n">gamespot</span> <span class="o">=</span> <span class="n">newspaper</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="s">'https://www.gamespot.com/news/'</span><span class="p">,</span> <span class="n">memoize_articles</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
    <span class="c1"># I set memoize_articles to False, because I don't want it to cache and save
</span>    <span class="c1"># articles run after run. Fresh run, every time essentially
</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">each_article</span> <span class="ow">in</span> <span class="n">gamespot</span><span class="p">.</span><span class="n">articles</span><span class="p">:</span>

    		<span class="n">each_article</span><span class="p">.</span><span class="n">download</span><span class="p">()</span>
    		<span class="n">each_article</span><span class="p">.</span><span class="n">parse</span><span class="p">()</span>
    		<span class="n">each_article</span><span class="p">.</span><span class="n">nlp</span><span class="p">()</span>

    		<span class="n">temp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Title'</span><span class="p">,</span> <span class="s">'Authors'</span><span class="p">,</span>
                                          <span class="s">'Text'</span><span class="p">,</span><span class="s">'Summary'</span><span class="p">,</span>
                                          <span class="s">'published_date'</span><span class="p">,</span><span class="s">'Source'</span><span class="p">])</span>

        <span class="n">temp_df</span><span class="p">[</span><span class="s">'Authors'</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_article</span><span class="p">.</span><span class="n">authors</span>
        <span class="n">temp_df</span><span class="p">[</span><span class="s">'Title'</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_article</span><span class="p">.</span><span class="n">title</span>
        <span class="n">temp_df</span><span class="p">[</span><span class="s">'Text'</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_article</span><span class="p">.</span><span class="n">text</span>
        <span class="n">temp_df</span><span class="p">[</span><span class="s">'Summary'</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_article</span><span class="p">.</span><span class="n">summary</span>
        <span class="n">temp_df</span><span class="p">[</span><span class="s">'published_date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_article</span><span class="p">.</span><span class="n">publish_date</span>
        <span class="n">temp_df</span><span class="p">[</span><span class="s">'Source'</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_article</span><span class="p">.</span><span class="n">source_url</span>

        <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_df</span><span class="p">,</span> <span class="n">ignore_index</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

    <span class="c1">#From here you can export this
</span>    <span class="n">final_df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">my_scraped_articles</span><span class="p">.</span><span class="n">csv</span><span class="p">)</span></code></pre></figure>

<p>….and there you go! That’s is how you scrape a bunch of articles easily. With the code above, you could implement a for loop to loop over a bunch of newspaper sources. Creating a massive final data-frame, that you could export and then play around with.</p>

<h3 id="enthusiast-multithreading-web-scraping">Enthusiast: Multithreading Web Scraping</h3>

<p>However, my proposed solution above could be a bit slow for some, as it downloads each article one after another. If you have many news sources, this could be a bit time consuming however, lets figure out a way to speed this all up. We can do this with a little help from <a href="https://realpython.com/intro-to-python-threading/">multithreading</a> technologies.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="kn">import</span> <span class="nn">newspaper</span>
    <span class="kn">from</span> <span class="nn">newspaper</span> <span class="kn">import</span> <span class="n">Article</span>
    <span class="kn">from</span> <span class="nn">newspaper</span> <span class="kn">import</span> <span class="n">Source</span>
    <span class="kn">from</span> <span class="nn">newspaper</span> <span class="kn">import</span> <span class="n">news_pool</span>
    <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

    <span class="n">gamespot</span> <span class="o">=</span> <span class="n">newspaper</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="s">'https://www.gamespot.com/news/'</span><span class="p">,</span> <span class="n">memoize_articles</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">bbc</span> <span class="o">=</span> <span class="n">newspaper</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="s">"https://www.bbc.com/news"</span><span class="p">,</span> <span class="n">memoize_articles</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">papers</span> <span class="o">=</span> <span class="p">[</span><span class="n">gamespot</span><span class="p">,</span> <span class="n">bbc</span><span class="p">]</span>
    <span class="n">news_pool</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">papers</span><span class="p">,</span> <span class="n">threads_per_source</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">news_pool</span><span class="p">.</span><span class="n">join</span><span class="p">()</span>

    <span class="c1">#Create our final dataframe
</span>    <span class="n">final_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1">#Create a download limit per sources
</span>    <span class="n">limit</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="k">for</span> <span class="n">source</span> <span class="ow">in</span> <span class="n">papers</span><span class="p">:</span>
        <span class="c1">#tempoary lists to store each element we want to extract
</span>        <span class="n">list_title</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">list_text</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">list_source</span> <span class="o">=</span><span class="p">[]</span>

        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">article_extract</span> <span class="ow">in</span> <span class="n">source</span><span class="p">.</span><span class="n">articles</span><span class="p">:</span>
            <span class="n">article_extract</span><span class="p">.</span><span class="n">parse</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span> <span class="c1">#Lets have a limit, so it doesnt take too long when you're
</span>                <span class="k">break</span>         <span class="c1">#running the code.
</span>
            <span class="c1">#appending the elements we want to extract
</span>            <span class="n">list_title</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">article_extract</span><span class="p">.</span><span class="n">title</span><span class="p">)</span>
            <span class="n">list_text</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">article_extract</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>
            <span class="n">list_source</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">article_extract</span><span class="p">.</span><span class="n">source_url</span><span class="p">)</span>

            <span class="c1">#Update count
</span>            <span class="n">count</span> <span class="o">+=</span><span class="mi">1</span>


        <span class="n">temp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'Title'</span><span class="p">:</span> <span class="n">list_title</span><span class="p">,</span> <span class="s">'Text'</span><span class="p">:</span> <span class="n">list_text</span><span class="p">,</span> <span class="s">'Source'</span><span class="p">:</span> <span class="n">list_source</span><span class="p">})</span>
        <span class="c1">#Append to the final DataFrame
</span>        <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_df</span><span class="p">,</span> <span class="n">ignore_index</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span></code></pre></figure>

<p>I’m a person who <strong>learns by doing</strong>, so I suggest anyone reading this to <strong>play with the code above.</strong> From here, you now be able to web scape articles using the newspaper3k plugin. <strong>Happy Web Scaping!</strong></p>

  </article>
</div>

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=https://andrewhnberry.github.io/articles/2020-04/The-Easy-Way-to-Web-Scrape-Articles-Online" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=https://andrewhnberry.github.io/articles/2020-04/The-Easy-Way-to-Web-Scrape-Articles-Online" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=https://andrewhnberry.github.io/articles/2020-04/The-Easy-Way-to-Web-Scrape-Articles-Online" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=https://andrewhnberry.github.io/articles/2020-04/The-Easy-Way-to-Web-Scrape-Articles-Online" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=https://andrewhnberry.github.io/articles/2020-04/The-Easy-Way-to-Web-Scrape-Articles-Online" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->


      </div>
    </div>
  </div>


</body>
</html>